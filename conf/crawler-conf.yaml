
# Custom configuration for StormCrawler
# This is used to override the default values from crawler-default.xml and provide additional ones 
# for your custom components.
# Use this file with the parameter -config when launching your extension of ConfigurableTopology.  
# This file does not contain all the key values but only the most frequently used ones. See crawler-default.xml for an extensive list.

config: 
  topology.workers: 1
  topology.message.timeout.secs: 300
  topology.max.spout.pending: 100
  topology.debug: false

  topology.name: "NewsCrawl"

  # mandatory when using Flux
  topology.kryo.register:
    - com.digitalpebble.stormcrawler.Metadata

  topology.backpressure.enable: false
  
  # set to 0 to deactivate debugging
  topology.eventlogger.executors: 0
  
  #Metrics consumers:
  topology.metrics.consumer.register:
     - class: "org.apache.storm.metric.LoggingMetricsConsumer"
       parallelism.hint: 1
     - class: "com.digitalpebble.stormcrawler.elasticsearch.metrics.MetricsConsumer"
       parallelism.hint: 1

  partition.url.mode: "byDomain"

  http.content.limit: 1048576
  
  http.store.headers: true

  urlfilters.config.file: "urlfilters.json"
  
  # time bucket to use for the metrics sent by the Fetcher
  fetcher.metrics.time.bucket.secs: 60
  
  fetcher.threads.number: 50

  worker.heap.memory.mb: 4096
  
  # delay between successive requests to the same host
  # (be defensive, a delay of 5 sec. means about 1000 fetches per hour
  #  which should be enough even for large news sites)
  fetcher.server.delay: 3.0

  # generous max. crawl delay
  # (fetch content even if the robots.txt specifies a large host-specific crawl delay:
  #  waiting 90 sec. between successive fetch would still allow to fetch
  #  about 1000 pages per day, enough for small news sites)
  fetcher.max.crawl.delay: 90

  # revisit a page (value in minutes)
  # - never revisit a news article
  #   (seeds / RSS feeds are revisited)
  # - 10 years (-1 would not allow to distinguish the fetch time, required for clean-ups)
  fetchInterval.default: 5256000
  
  # revisit a page with a fetch error after 2 hours (value in minutes)
  fetchInterval.fetch.error: 120
  
  # revisit a page with an error after 3 month (value in minutes)
  fetchInterval.error: 133920

  # if the url is a feed then revisit it 10 mins later
  #   fetchInterval.isFeed=true: 10
  # ... 3h later should be enough (8 requests per day)
  fetchInterval.isFeed=true: 180
  
  # news sitemaps may contain up 1000 URLs, fetch twice per day
  fetchInterval.isSitemapNews=true: 720

  # auto-detect RSS feeds
  feed.sniffContent: true

  # auto-detection of news sitemaps
  sitemap.sniffContent: true
  sitemap.discovery: true

  status.updater.cache.spec: "maximumSize=250000,expireAfterAccess=4h"

  # lists the metadata to persist to storage
  # these are not transfered to the outlinks
  metadata.persist:
   - _redirTo
   - error.cause
   - error.source
   - isSitemap
   - isSitemapNews
   - isFeed
   - last-modified

  # HTTP 'User-Agent' request header
  # NOTE: must set these configuration properties to identify your bot!
  http.agent.name: ""
  http.agent.version: ""
  http.agent.description: ""
  http.agent.url: ""
  http.agent.email: ""

  # change to the location of your choice
  # the directory must already exist
  warc.dir: "/data/warc"


