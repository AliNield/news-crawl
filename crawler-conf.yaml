# Custom configuration for StormCrawler
# This is used to override the default values from crawler-default.xml and provide additional ones 
# for your custom components.
# Use this file with the parameter -config when launching your extension of ConfigurableTopology.  
# This file does not contain all the key values but only the most frequently used ones. See crawler-default.xml for an extensive list.

config: 
  topology.workers: 1
  topology.message.timeout.secs: 300
  topology.max.spout.pending: 100
  topology.debug: false

  topology.name: "NewsCrawl"

  # mandatory when using Flux
  topology.kryo.register:
    - com.digitalpebble.stormcrawler.Metadata

  topology.backpressure.enable: false
  
  # set to 0 to deactivate debugging
  topology.eventlogger.executors: 0
  
  #Metrics consumers:
  topology.metrics.consumer.register:
     - class: "org.apache.storm.metric.LoggingMetricsConsumer"
       parallelism.hint: 1
     - class: "com.digitalpebble.stormcrawler.elasticsearch.metrics.MetricsConsumer"
       parallelism.hint: 1

  partition.url.mode: "byDomain"

  http.content.limit: 1048576
  
  urlfilters.config.file: "urlfilters.json"
  
  # time bucket to use for the metrics sent by the Fetcher
  fetcher.metrics.time.bucket.secs: 30
  
  fetcher.threads.number: 20
  
  # revisit a page monthly (value in minutes)
  fetchInterval.default: 44640
  
  # revisit a page with a fetch error after 2 hours (value in minutes)
  fetchInterval.fetch.error: 120
  
  # revisit a page with an error every month (value in minutes)
  fetchInterval.error: 44640

  # if the url is a feed then revisit it 10 mins later
  fetchInterval.isFeed=true: 10
  
  status.updater.cache.spec: "maximumSize=250000,expireAfterAccess=4h"

  # lists the metadata to persist to storage
  # these are not transfered to the outlinks
  metadata.persist:
   - _redirTo
   - error.cause
   - error.source
   - isSitemap
   - isFeed

  http.agent.name: "NewsCrawler"
  http.agent.version: "1.0"
  http.agent.description: "A StormCrawler-based crawler for newsfeeds"
  http.agent.url: "http://commoncrawl.org/"
  http.agent.email: "sebastian@commoncrawl.org"

  # change to the location of your choice
  # the directory must already exist
  warc.dir: "/data/warc"


